{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10b0af80",
   "metadata": {},
   "source": [
    "# Simplified Transformer (implementation)\n",
    "This Jupyter notebook implements the **simplified transformer** described in the provided PDF (\"Extracting LTL formulas from Transformers\") â€” a leftmost-hard-max single-head transformer used for Boolean word classification.\n",
    "\n",
    "Features implemented:\n",
    "- Token encoding matrix `Wenc` (no two rows equal)\n",
    "- Leftmost-argmax attention per row: `A = LArgMax(X Q (X K)^T)`\n",
    "- Layer update: `X' = phi( A (X V) O ) + X`\n",
    "- Output classification: `True` iff final layer last-row first-column `> theta`\n",
    "- Options for deterministic weights (seeded) and custom activation\n",
    "\n",
    "The cells below include the full implementation, a demo, and a few unit tests/examples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53b8ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import List, Tuple, Dict, Any\n",
    "\n",
    "\n",
    "def leftmost_argmax_rows(matrix: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    For each row of `matrix`, return a one-hot row vector with a 1 at\n",
    "    the leftmost index of the maximum value in that row.\n",
    "    Input: matrix shape (n, n) or (n, m)\n",
    "    Output: one-hot matrix shape (n, m)\n",
    "    \"\"\"\n",
    "    if matrix.ndim != 2:\n",
    "        raise ValueError(\"matrix must be 2D\")\n",
    "    # np.argmax returns index of first occurrence of max along axis\n",
    "    idx = np.argmax(matrix, axis=1)\n",
    "    one_hot = np.zeros_like(matrix)\n",
    "    rows = np.arange(matrix.shape[0])\n",
    "    one_hot[rows, idx] = 1\n",
    "    return one_hot\n",
    "\n",
    "# Activation helpers\n",
    "def get_activation(name: str):\n",
    "    if name == 'relu':\n",
    "        return lambda x: np.maximum(0, x)\n",
    "    elif name == 'tanh':\n",
    "        return np.tanh\n",
    "    elif name == 'identity':\n",
    "        return lambda x: x\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown activation: {name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c714a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimplifiedTransformer:\n",
    "    \"\"\"\n",
    "    Implements the simplified transformer as in the PDF.\n",
    "\n",
    "    - vocab: list of symbols (should include '!' symbol separately if used)\n",
    "    - d: feature dimension\n",
    "    - d0: internal attention/value dimension\n",
    "    - L: number of layers\n",
    "    - activation: 'relu', 'tanh', or 'identity'\n",
    "    - theta: classification threshold\n",
    "    - seed: optionally set random seed for deterministic weights\n",
    "\n",
    "    The parameters Q/K/V/O per layer and Wenc are numpy arrays.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 vocab: List[str],\n",
    "                 d: int = 8,\n",
    "                 d0: int = 4,\n",
    "                 L: int = 2,\n",
    "                 activation: str = 'relu',\n",
    "                 theta: float = 0.0,\n",
    "                 seed: int = None,\n",
    "                 Wenc: np.ndarray = None,\n",
    "                 Qs: List[np.ndarray] = None,\n",
    "                 Ks: List[np.ndarray] = None,\n",
    "                 Vs: List[np.ndarray] = None,\n",
    "                 Os: List[np.ndarray] = None):\n",
    "        self.vocab = list(vocab)\n",
    "        self.vocab_index = {tok: i for i, tok in enumerate(self.vocab)}\n",
    "        self.d = d\n",
    "        self.d0 = d0\n",
    "        self.L = L\n",
    "        self.activation_name = activation\n",
    "        self.phi = get_activation(activation)\n",
    "        self.theta = theta\n",
    "        self.rng = np.random.RandomState(seed)\n",
    "\n",
    "        # Encoding matrix: |vocab| x d, no two rows equal\n",
    "        if Wenc is None:\n",
    "            # construct deterministic rows with small variations to avoid equality\n",
    "            base = self.rng.randn(len(self.vocab), d)\n",
    "            # ensure rows are unique by adding small offsets\n",
    "            for i in range(len(self.vocab)):\n",
    "                base[i] += (i * 1e-3)\n",
    "            self.Wenc = base\n",
    "        else:\n",
    "            assert Wenc.shape == (len(self.vocab), d)\n",
    "            self.Wenc = Wenc\n",
    "\n",
    "        # Per-layer matrices\n",
    "        def rand_mat(shape):\n",
    "            return self.rng.randn(*shape)\n",
    "\n",
    "        self.Qs = Qs if Qs is not None else [rand_mat((d, d0)) for _ in range(L)]\n",
    "        self.Ks = Ks if Ks is not None else [rand_mat((d, d0)) for _ in range(L)]\n",
    "        self.Vs = Vs if Vs is not None else [rand_mat((d, d0)) for _ in range(L)]\n",
    "        self.Os = Os if Os is not None else [rand_mat((d0, d)) for _ in range(L)]\n",
    "\n",
    "    def encode(self, word: List[str]) -> np.ndarray:\n",
    "        \"\"\"Return X0: n x d matrix for the input admissible word (list of tokens).\"\"\"\n",
    "        rows = []\n",
    "        for tok in word:\n",
    "            if tok not in self.vocab_index:\n",
    "                raise ValueError(f\"Token '{tok}' not in vocab\")\n",
    "            rows.append(self.Wenc[self.vocab_index[tok]])\n",
    "        return np.vstack(rows)\n",
    "\n",
    "    def forward(self, word: List[str], return_all: bool = False) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Run the transformer on an admissible word (list of tokens).\n",
    "        Returns a dict with keys:\n",
    "          - 'X_layers': list of X matrices (including X0)\n",
    "          - 'A_layers': list of attention one-hot matrices per layer\n",
    "          - 'classification': boolean result\n",
    "          - 'score': final value XL[last_row, 0]\n",
    "        \"\"\"\n",
    "        X = self.encode(word)  # X0\n",
    "        n = X.shape[0]\n",
    "        X_layers = [X.copy()]\n",
    "        A_layers = []\n",
    "\n",
    "        for ell in range(self.L):\n",
    "            Q = self.Qs[ell]\n",
    "            K = self.Ks[ell]\n",
    "            V = self.Vs[ell]\n",
    "            O = self.Os[ell]\n",
    "\n",
    "            # compute scores: (X Q) (X K)^T  -> shape n x n\n",
    "            scores = (X.dot(Q)).dot((X.dot(K)).T)\n",
    "            A = leftmost_argmax_rows(scores)\n",
    "            A_layers.append(A)\n",
    "\n",
    "            # compute A (X V) O\n",
    "            XV = X.dot(V)        # n x d0\n",
    "            A_XV = A.dot(XV)     # n x d0\n",
    "            Y = A_XV.dot(O)      # n x d\n",
    "            X = self.phi(Y) + X  # residual\n",
    "            X_layers.append(X.copy())\n",
    "\n",
    "        final_score = X[-1, 0]  # XL[n,1] in 1-based indexing\n",
    "        classification = bool(final_score > self.theta)\n",
    "\n",
    "        out = {\n",
    "            'X_layers': X_layers,\n",
    "            'A_layers': A_layers,\n",
    "            'classification': classification,\n",
    "            'score': final_score,\n",
    "            'word': word\n",
    "        }\n",
    "        if return_all:\n",
    "            return out\n",
    "        else:\n",
    "            return {'classification': classification, 'score': final_score}\n",
    "\n",
    "    def pretty_print_trace(self, trace: Dict[str, Any]):\n",
    "        print(f\"Input word: {' '.join(trace['word'])}\")\n",
    "        for i, X in enumerate(trace['X_layers']):\n",
    "            print(f\"\\nX layer {i} (shape {X.shape}):\")\n",
    "            print(np.round(X, 4))\n",
    "            if i < len(trace['A_layers']):\n",
    "                print(f\"A layer {i+1}:\")\n",
    "                print(trace['A_layers'][i])\n",
    "        print(f\"\\nFinal score (last-row, first-col): {trace['score']:.6f}\")\n",
    "        print(f\"Classification (> {self.theta}): {trace['classification']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0fccb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo: build transformer and run on a few example admissible words\n",
    "vocab = ['a', 'b', 'c', '!']\n",
    "transformer = SimplifiedTransformer(vocab=vocab, d=8, d0=4, L=3, activation='relu', theta=0.0, seed=42)\n",
    "\n",
    "examples = [\n",
    "    ['a','a','!'],\n",
    "    ['a','b','!'],\n",
    "    ['b','b','!'],\n",
    "    ['c','a','b','!']\n",
    "]\n",
    "\n",
    "for w in examples:\n",
    "    trace = transformer.forward(w, return_all=True)\n",
    "    transformer.pretty_print_trace(trace)\n",
    "    print('\\n' + '-'*60 + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c05549",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unit-test style example: craft Wenc and layer matrices so attention picks a predictable token\n",
    "vocab = ['x', 'y', '!']\n",
    "# choose d=3, d0=2 for clarity\n",
    "Wenc = np.array([[1.0, 0.0, 0.0],  # x\n",
    "                 [0.0, 1.0, 0.0],  # y\n",
    "                 [0.0, 0.0, 1.0]]) # !\n",
    "\n",
    "# design Q and K so that tokens with matching one-hot attend to each other\n",
    "Q = np.array([[1.0,0.0],[0.0,1.0],[0.0,0.0]])  # d x d0\n",
    "K = Q.copy()\n",
    "V = np.array([[1.0,0.0],[0.0,1.0],[0.0,0.0]])  # identity-ish\n",
    "O = np.array([[1.0,0.0,0.0],[0.0,1.0,0.0]])      # d0 x d -> will reconstruct\n",
    "\n",
    "# single layer transformer with known behavior\n",
    "T = SimplifiedTransformer(vocab=vocab, d=3, d0=2, L=1, activation='identity', theta=0.5, seed=0,\n",
    "                          Wenc=Wenc, Qs=[Q], Ks=[K], Vs=[V], Os=[O])\n",
    "\n",
    "# if word ends with '!' and we want classifier to check last-row first-col, we can craft\n",
    "w1 = ['x', '!']\n",
    "w2 = ['y', '!']\n",
    "\n",
    "trace1 = T.forward(w1, return_all=True)\n",
    "trace2 = T.forward(w2, return_all=True)\n",
    "\n",
    "print('Trace for', w1)\n",
    "T.pretty_print_trace(trace1)\n",
    "print('\\nTrace for', w2)\n",
    "T.pretty_print_trace(trace2)\n",
    "\n",
    "print('\\nNote: with identity activation and the chosen matrices, you can reason about which positions each row attends to and how the final value changes.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b97da01",
   "metadata": {},
   "source": [
    "## Notes\n",
    "- This notebook follows the exact formulas in the PDF: attention uses leftmost-argmax, single-head.\n",
    "- No positional encoding was added (as in the PDF).\n",
    "- You can experiment by changing seeds, layer count `L`, dimensions `d` and `d0`, and activation.\n",
    "\n",
    "## How to run\n",
    "1. Download the notebook file and open it in Jupyter (or JupyterLab / VS Code).\n",
    "2. Run all cells.\n",
    "\n",
    "Enjoy â€” if you'd like, I can:\n",
    "- add more visualizations (plot attention patterns),\n",
    "- convert this to a ready-to-run `.py` module or a hosted Google Colab link,\n",
    "- or extend it to multi-head / softmax attention.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
